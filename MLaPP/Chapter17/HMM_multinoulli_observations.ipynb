{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Hidden Markov Model with categorical observations.\n",
    "Use Expectation-Maximization to find the parameters.\n",
    "\n",
    "The expectation step uses forward-backward algorithm\n",
    "\n",
    "Notations follow Chapter 17, MLaPP, Kevin Murphy\n",
    "\n",
    "Example: use historical `bullish to bearish ratio` obtained from American Association of Individual Investors website as an example data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula used:\n",
    "    \n",
    "Forward:\n",
    "$\n",
    "\\mathbf{\\alpha}_t \\propto \\mathbf{\\phi}_t \\odot (A^T \\mathbf{\\alpha}_{t-1})  \\hspace{500pt}\n",
    "$\n",
    "\n",
    "Backward:\n",
    "$\n",
    "\\mathbf{\\beta}_t = A (\\mathbf{\\phi}_{t+1} \\odot \\mathbf{\\beta}_{t+1})   \\hspace{500pt}\n",
    "$\n",
    "\n",
    "Forward-Backward:\n",
    "\n",
    "1) $\\gamma_t(j) \\equiv P(z_t=j | x_{1:T})$  \\hsapce{500pt}\n",
    "\n",
    "$\n",
    "\\mathbf{\\gamma}_t \\propto \\alpha_t \\odot \\beta_t   \\hspace{500pt}\n",
    "$\n",
    "    \n",
    "2) two-slice marginal\n",
    "\n",
    "$\n",
    "\\xi_{t,t+1} \\propto A \\odot \\big(\\alpha_t ( \\phi_{t+1} \\odot \\beta_{t+1})^T \\big)   \\hspace{500pt}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \n",
    "    def __init__(self, obs, num_states):\n",
    "        self.obs = obs\n",
    "        self.K = num_states\n",
    "        self.m = obs.max() + 1\n",
    "        \n",
    "    def normalize(self, p):\n",
    "        norm = np.sum(p)\n",
    "        p = p/norm\n",
    "        return p, norm\n",
    "    \n",
    "    def forward(self, pi, A, B, obs):\n",
    "        # pi: lenght-K, prior distribution over K states\n",
    "        # A: KxK transition matrix, A[i, j] is the transition probability from state i to state j\n",
    "        # B: mxK, each column is a state specific distribution over observations\n",
    "        # obs: one given observation        \n",
    "        K = A.shape[0]\n",
    "        T = obs.size\n",
    "        alpha = np.zeros((K, T))\n",
    "        norms = np.zeros(T)\n",
    "        \n",
    "        alpha[:, 0] = B[obs[0], :] * pi\n",
    "        alpha[:, 0], norms[0] = self.normalize(alpha[:, 0])\n",
    "        for t in range(1, T):\n",
    "            alpha[:, t] = B[obs[t], :] * (A.T.dot(alpha[:, t-1]))\n",
    "            alpha[:, t], norms[t] = self.normalize(alpha[:, t])\n",
    "        return alpha, norms\n",
    "    \n",
    "    def backward(self, pi, A, B, obs, alpha, norms):\n",
    "        # pi: lenght-K, prior distribution over K states\n",
    "        # A: KxK transition matrix, A[i, j] is the transition probability from state i to state j\n",
    "        # B: mxK, each column is a state specific distribution over observations\n",
    "        # obs: one given observation        \n",
    "        # alpha: from the forward algorithm        \n",
    "        K = A.shape[0]\n",
    "        T = obs.size\n",
    "        beta = np.zeros((K, T))\n",
    "        beta[:, T-1] = 1\n",
    "        beta[:, T-1] = beta[:, T-1] / norms[T-1]\n",
    "        for t in range(T-2, -1, -1):\n",
    "            beta[:, t] = A.dot(B[obs[t+1], : ] * beta[:, t+1])\n",
    "            beta[:, t] = beta[:, t] / norms[t]\n",
    "            # beta[:, t] is normalized with the same factor that normalized alpha[:, t], this\n",
    "            #  ensures that gamma[:, t] is normalized\n",
    "        return beta\n",
    "    \n",
    "    def forward_backward(self, A, B, obs, alpha, beta):\n",
    "        # A: KxK transition matrix, A[i, j] is the transition probability from state i to state j\n",
    "        # B: mxK, each column is a state specific distribution over observations\n",
    "        # obs: one given observation\n",
    "        # alpha: from the forward algorithm        \n",
    "        # beta: from the backward algorithm\n",
    "        K = A.shape[0]\n",
    "        T = obs.size\n",
    "        gamma = np.zeros((K, T))\n",
    "        for t in range(T):\n",
    "            gamma[:, t] = alpha[:, t] * beta[:, t]\n",
    "            gamma[:, t], _ = self.normalize(gamma[:, t])\n",
    "        \n",
    "        Xi = np.zeros((K, K))\n",
    "        for t in range(T-1):\n",
    "            tmp = A * np.outer(alpha[:, t], B[obs[t+1], :] * beta[:, t+1])\n",
    "            tmp = tmp/np.sum(tmp)\n",
    "            Xi += tmp\n",
    "        \n",
    "        return gamma, Xi\n",
    "    \n",
    "    def expectation(self, pi, A, B, obs):\n",
    "        alpha, norms = self.forward(pi, A, B, obs)\n",
    "        beta = self.backward(pi, A, B, obs, alpha, norms)\n",
    "        gamma, Xi = self.forward_backward(A, B, obs, alpha, beta)\n",
    "        return alpha, norms, beta, gamma, Xi\n",
    "    \n",
    "    def maximization(self, gamma, Xi, obs):\n",
    "        K = self.K\n",
    "        m = self.m\n",
    "        T = obs.size\n",
    "        \n",
    "        pi = gamma[:, 0].copy()\n",
    "        \n",
    "        A = Xi.copy()\n",
    "        for i in range(K):\n",
    "            A[i, :], _ = self.normalize(A[i, :])\n",
    "            \n",
    "        B = np.zeros((m, K))\n",
    "        for l in range(m):\n",
    "            for t in range(T):\n",
    "                if obs[t] == l:\n",
    "                    B[l, :] += gamma[:, t]\n",
    "        for j in range(K):\n",
    "            B[:, j], _ = self.normalize(B[:, j])\n",
    "        \n",
    "        return pi, A, B\n",
    "    \n",
    "    def train_EM(self, n_iters=1000):\n",
    "        \n",
    "        printing_frequency = n_iters // 10\n",
    "        \n",
    "        # initialization\n",
    "        K = self.K\n",
    "        m = self.m\n",
    "        obs = self.obs\n",
    "        T = obs.size\n",
    "        \n",
    "        # Initialization\n",
    "        pi = np.random.random(K)\n",
    "        pi, _ = self.normalize(pi)\n",
    "        \n",
    "        A = np.random.random((K, K))\n",
    "        for j in range(K):\n",
    "            A[j, :], _ = self.normalize(A[j, :])\n",
    "\n",
    "        B = np.random.random((m, K))\n",
    "        for j in range(K):\n",
    "            B[:, j], _ = self.normalize(B[:, j])\n",
    "            \n",
    "        # EM iterations    \n",
    "        for counter in range(n_iters):            \n",
    "            alpha, norms, beta, gamma, Xi = self.expectation(pi, A, B, obs)\n",
    "            pi, A, B = self.maximization(gamma, Xi, obs)\n",
    "            if counter % printing_frequency == 0:\n",
    "                cost_function = -np.sum(np.log(norms))\n",
    "                print('iteration: {0:}, cost function: {1:}'.format(counter, cost_function))\n",
    "        return pi, A, B\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(obs, pi, A, B):\n",
    "    #\n",
    "    # assume that there are K hidden states\n",
    "    #   each state has a distribution over M possible outputs, which we observe\n",
    "    #\n",
    "    # Inputs:\n",
    "    #     obs: observations from a sequence of length T\n",
    "    #\n",
    "    #     pi: K, prior distribution over states    \n",
    "    #\n",
    "    #     A: transition matrix for the hidden state, KxK\n",
    "    #\n",
    "    #     B : MxK, state specific distribution over possible outputs\n",
    "    #       #\n",
    "    # Internal variables:\n",
    "    #     delta: KxT\n",
    "    #         delta[j, t]: the most probable probability for state j at time t\n",
    "    #\n",
    "    #     a: KxT\n",
    "    #        a[j, t]: the most probable state at time (t-1) that would lead to state j at time t\n",
    "    \n",
    "    T = obs.size\n",
    "    K = A.shape[0]\n",
    "    \n",
    "    delta = np.zeros((K, T))\n",
    "    a = np.zeros((K, T))\n",
    "    \n",
    "    delta[:, 0] = pi * B[obs[0], :]\n",
    "    for t in range(1, T):\n",
    "        for j in range(K):\n",
    "            delta[j, t] = np.max(delta[:, t-1] * A[:, j] * B[obs[t], j])\n",
    "            a[j, t] = np.argmax(delta[:, t-1] * A[:, j] * B[obs[t], j])\n",
    "            \n",
    "    # traceback \n",
    "    path = np.zeros(T)\n",
    "    path[T-1] = np.argmax(delta[:, T-1])\n",
    "    for t in range(T-2, -1, -1):\n",
    "        path[t] = a[int(path[t+1]), t+1]\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.aaii.com/sentimentsurvey/sent_results\n",
    "data = pd.read_csv('AAII_raw.csv', header=0)\n",
    "\n",
    "data['ratio'] = data.eval('Bullish / Bearish')\n",
    "obs_max = data['ratio'].max()\n",
    "obs_min = data['ratio'].min()\n",
    "obs_delta = obs_max - obs_min\n",
    "\n",
    "data['scaled_obs'] = data['ratio'].apply(lambda x: (x - obs_min)/obs_delta)\n",
    "buckets = np.linspace(0, 1, 6)\n",
    "data['digitized_obs'] = np.digitize(data['scaled_obs'], buckets) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Bullish</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Bearish</th>\n",
       "      <th>ratio</th>\n",
       "      <th>scaled_obs</th>\n",
       "      <th>digitized_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7-24-87</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7-31-87</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8-7-87</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8-14-87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8-21-87</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  Bullish  Neutral  Bearish  ratio  scaled_obs  digitized_obs\n",
       "0  7-24-87      0.4      0.5      0.1    4.0    0.490909              2\n",
       "1  7-31-87      0.3      0.5      0.3    1.0    0.109091              0\n",
       "2   8-7-87      0.6      0.2      0.3    2.0    0.236364              1\n",
       "3  8-14-87      0.5      0.4      0.2    2.5    0.300000              1\n",
       "4  8-21-87      0.7      0.3      0.1    7.0    0.872727              4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs = data['digitized_obs'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, cost function: 3613.877366241577\n",
      "iteration: 100, cost function: 925.5114808356143\n",
      "iteration: 200, cost function: 925.2406293772912\n",
      "iteration: 300, cost function: 925.1426442976081\n",
      "iteration: 400, cost function: 924.8978398098704\n",
      "iteration: 500, cost function: 920.9800093358799\n",
      "iteration: 600, cost function: 919.0483644111841\n",
      "iteration: 700, cost function: 918.4930880828682\n",
      "iteration: 800, cost function: 914.3994293432136\n",
      "iteration: 900, cost function: 913.4424556023216\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(obs, 5)\n",
    "pi, A, B = hmm.train_EM(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201   0.0   0.0785   0.0014   0.0   \n",
      "0.0   0.6494   0.3506   0.0001   0.0   \n",
      "0.1329   0.0   0.7484   0.1187   0.0   \n",
      "0.0137   0.1544   0.0005   0.6867   0.1447   \n",
      "0.0   0.0   0.0   0.3359   0.6641   \n"
     ]
    }
   ],
   "source": [
    "for i in range(A.shape[0]):\n",
    "    row = ''\n",
    "    for e in A[i, :]:\n",
    "        e_str =  str(round(e, 4))\n",
    "        row += e_str + '   '\n",
    "    print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0   0.9361   0.6727   0.2724   0.0674   \n",
      "0.0   0.0015   0.3273   0.7276   0.5678   \n",
      "0.0   0.0   0.0   0.0   0.1068   \n",
      "0.0   0.0482   0.0   0.0   0.2065   \n",
      "0.0   0.0142   0.0   0.0   0.0351   \n",
      "0.0   0.0   0.0   0.0   0.0164   \n"
     ]
    }
   ],
   "source": [
    "for i in range(B.shape[0]):\n",
    "    row = ''\n",
    "    for e in B[i, :]:\n",
    "        e_str =  str(round(e, 4))\n",
    "        row += e_str + '   '\n",
    "    print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_probable_sequence = viterbi(obs, pi, A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
