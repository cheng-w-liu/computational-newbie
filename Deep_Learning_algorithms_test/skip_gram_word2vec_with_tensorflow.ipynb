{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note book demonstrates skip-gram word2vec using TensorFlow. \n",
    "\n",
    "This is motivated by an awesome blogplot: http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/ , with some variation in Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import os\n",
    "import six\n",
    "import urllib\n",
    "import zipfile\n",
    "import datetime\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybeDownload(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as fh:\n",
    "        raw_data = tf.compat.as_str(fh.read(fh.namelist()[0])).split()    \n",
    "    return raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildDataset(words, topK=10000):\n",
    "    word_counts = [['UNK', -1]]\n",
    "    word_counts.extend(collections.Counter(words).most_common(topK-1))\n",
    "    word2idx = {}\n",
    "    for w, _ in word_counts:\n",
    "        word2idx[w] = len(word2idx)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for w in words:\n",
    "        if w in word2idx:\n",
    "            idx = word2idx[w]\n",
    "        else:\n",
    "            idx = 0\n",
    "            unk_count += 1\n",
    "        data.append(idx)\n",
    "    word_counts[0][1] = unk_count\n",
    "    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "    return data, word_counts, word2idx, idx2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collectData(vocabulary_size=10000, remove_stop_words=True):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybeDownload('text8.zip', url, 31344016)\n",
    "    raw_data = readData(filename)\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        raw_data = [w for w in raw_data if w not in stop_words]\n",
    "    data, word_counts, word2idx, idx2word = buildDataset(raw_data, vocabulary_size)\n",
    "    del raw_data\n",
    "    return data, word_counts, word2idx, idx2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_idx = 0\n",
    "def generateBatch(data, batch_size, num_skips, window_size):\n",
    "    global data_idx\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2*window_size\n",
    "    input_words = np.ndarray(shape=(batch_size), dtype=np.int32)   # the context\n",
    "    output_words = np.ndarray(shape=(batch_size), dtype=np.int32)  # the target\n",
    "    span = 2 * window_size + 1\n",
    "    words_in_window = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        words_in_window.append(data[data_idx])\n",
    "        data_idx = (data_idx + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):    \n",
    "        input_word_idx = window_size\n",
    "        indices_to_avoid = [window_size]\n",
    "        for j in range(num_skips):\n",
    "            while input_word_idx in indices_to_avoid:\n",
    "                input_word_idx = np.random.randint(0, span)\n",
    "            indices_to_avoid.append(input_word_idx)\n",
    "            input_words[i * num_skips + j] = words_in_window[window_size]\n",
    "            output_words[i * num_skips + j] = words_in_window[input_word_idx]\n",
    "        words_in_window.append(data[data_idx])    \n",
    "        data_idx = (data_idx + 1) % len(data)\n",
    "    data_idx = (data_idx + len(data) - span) % len(data)\n",
    "    return input_words, output_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "data, word_counts, word2idx, idx2word = collectData(vocabulary_size, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_skips = 4\n",
    "window_size = 5\n",
    "# input_words, output_words = generateBatch(data, batch_size, num_skips, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_size = 16\n",
    "val_window = 100\n",
    "val_examples = np.random.choice(val_window, val_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "learning_rate = 0.01\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # create\n",
    "    training_inputs = tf.placeholder(dtype=tf.int32, shape=(batch_size))\n",
    "    training_outputs = tf.placeholder(dtype=tf.int32, shape=(batch_size))\n",
    "    val_inputs = tf.constant(val_examples, dtype=tf.int32)\n",
    "    \n",
    "    # construct the embedding matrix\n",
    "    embedding_matrix = tf.get_variable(name='embedding_matrix', dtype=tf.float64,\n",
    "                                       shape=[vocabulary_size, embedding_size],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, training_inputs)  # shape: [n_train, embedding_size]\n",
    "    \n",
    "    # construct the weight and bias for the softmax\n",
    "    W = tf.get_variable(name='W', dtype=tf.float64,\n",
    "                              shape=[vocabulary_size, embedding_size],\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(name='b', dtype=tf.float64,\n",
    "                        shape=[1, vocabulary_size],\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    \n",
    "    Z = tf.add(tf.matmul(embeddings, W, transpose_b=True), b)\n",
    "    \n",
    "    # convert the label to one-hot-encoded form\n",
    "    train_outptus_one_hot = tf.one_hot(training_outputs, vocabulary_size)\n",
    "    \n",
    "    # construct the objective function (cross entropy) and Adam optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z, labels=train_outptus_one_hot))    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    norm = tf.square(tf.reduce_sum(tf.square(embedding_matrix), 1, keep_dims=True))\n",
    "    normalized_embeddings = embedding_matrix / norm\n",
    "    val_embeddings = tf.nn.embedding_lookup(normalized_embeddings, val_inputs)\n",
    "    similarity = tf.matmul(val_embeddings, val_embeddings, transpose_b=True)\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k = 8\n",
    "def run(graph, num_iters, interval_size=100, print_valida_similarity=True):\n",
    "    training_history = []\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        init.run()\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        for step in range(num_iters):\n",
    "            batch_inputs, batch_outputs = generateBatch(data, batch_size, num_skips, window_size)\n",
    "            \n",
    "            feed_dict = {training_inputs: batch_inputs, training_outputs: batch_outputs}            \n",
    "            _ , batch_cost = session.run([optimizer, cost], feed_dict=feed_dict)\n",
    "            avg_loss += batch_cost\n",
    "            \n",
    "            if step > 0 and step % interval_size == 0:\n",
    "                avg_loss /= interval_size\n",
    "                training_history.append(avg_loss)\n",
    "                #print(\"step: {0}, avg. loss: {1:}\".format(step, avg_loss))\n",
    "                avg_loss = 0.0\n",
    "                              \n",
    "            if print_valida_similarity and step > 0 and step % 10*interval_size == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(val_size):\n",
    "                    val_word = idx2word[val_examples[i]]\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1] \n",
    "                    print('top {0:} words closest to {1:}'.format(top_k, val_word))\n",
    "                    for k in range(top_k):\n",
    "                        close_word = idx2word[nearest[k]]\n",
    "                        print('  ' + close_word)        \n",
    "                print('------------------')\n",
    "                    \n",
    "    return training_history            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3:09:47.107237\n"
     ]
    }
   ],
   "source": [
    "num_iters = min(10 * len(data), 50000)\n",
    "start_time = datetime.datetime.now()\n",
    "training_history = run(graph, num_iters=num_iters, print_valida_similarity=False)\n",
    "elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "print('elapsed time: {0:}'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
