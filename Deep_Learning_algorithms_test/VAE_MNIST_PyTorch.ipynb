{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivated by the following posts\n",
    "\n",
    "https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/#a-simple-vae-implemented-using-pytorch\n",
    "\n",
    "https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Auto Encoder \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=28*28, hidden_dim=400, latent_dim=20):\n",
    "        \"\"\"\n",
    "        @param input_dim: dimention of the input, expected to be the MNIST images \n",
    "        @param hidden_dim: dimension of the hidden fully connected layer, \n",
    "        @param latent_dim: dimension of the latent space, i.e., dimension of the mean and variance of the underlying Gaussian        \n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc2(h)\n",
    "        log_var = self.fc3(h)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparametrize(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * log_var) \n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return torch.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparametrize(mu, log_var)\n",
    "        x_tilde = self.decode(z)\n",
    "        return x_tilde, mu, log_var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True if torch.cuda.is_available() else False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 1\n",
    "BATCH_SIZE = 128\n",
    "LOG_INTERVAL = 10\n",
    "EPOCHS = 30\n",
    "ZDIMS = 20\n",
    "learning_rate = 0.003 #1e-3\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_train = datasets.MNIST(\n",
    "#    os.path.expanduser('~/ml_datasets/'),\n",
    "#    train=True,\n",
    "#    download=True,\n",
    "#    transform=transforms.ToTensor()\n",
    "#)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        os.path.expanduser('~/ml_datasets/'),\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **kwargs    \n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        os.path.expanduser('~/ml_datasets/'),\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    **kwargs    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_dim=28*28, hidden_dim=400, latent_dim=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x: torch.Tensor, x_tilde: torch.Tensor, mu: torch.Tensor, log_var: torch.Tensor):\n",
    "    reconstruct_err = F.binary_cross_entropy(x_tilde, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1.0 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return reconstruct_err + kl_div\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_idx) -> float:\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.view(-1, 28 * 28).to(device)\n",
    "        x_tilde, mu, log_var = model(x)        \n",
    "        loss = loss_func(x, x_tilde, mu, log_var)\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  \n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch_idx):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        x = data.view(-1, 28 * 28).to(device)\n",
    "        x_tilde, mu, log_var = model(x)\n",
    "        test_loss += loss_func(x, x_tilde, mu, log_var).item()\n",
    "        \n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            comparison = torch.cat([data[:n], x_tilde.view(-1, 1, 28, 28)[:n]])\n",
    "            save_image(comparison.data.cpu(), \n",
    "                       './mnist_vae_results/reconstruction_' + str(epoch_idx) + '.png',\n",
    "                       nrow=8)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 144.6460, train time: 13.67; test loss: 117.9572, test time: 1.24\n",
      "Epoch 1, train loss: 114.8587, train time: 14.42; test loss: 111.6151, test time: 1.24\n",
      "Epoch 2, train loss: 111.0970, train time: 15.64; test loss: 109.6300, test time: 1.25\n",
      "Epoch 3, train loss: 109.4085, train time: 15.92; test loss: 108.4468, test time: 1.26\n",
      "Epoch 4, train loss: 108.3950, train time: 15.85; test loss: 107.3400, test time: 1.24\n",
      "Epoch 5, train loss: 107.6463, train time: 16.12; test loss: 106.8236, test time: 1.33\n",
      "Epoch 6, train loss: 107.1626, train time: 15.91; test loss: 106.2297, test time: 1.34\n",
      "Epoch 7, train loss: 106.6156, train time: 16.06; test loss: 106.0287, test time: 1.31\n",
      "Epoch 8, train loss: 106.3203, train time: 16.02; test loss: 105.8030, test time: 1.33\n",
      "Epoch 9, train loss: 105.9918, train time: 15.97; test loss: 105.7344, test time: 1.31\n",
      "Epoch 10, train loss: 105.7012, train time: 16.12; test loss: 105.3562, test time: 1.34\n",
      "Epoch 11, train loss: 105.4922, train time: 16.21; test loss: 105.4538, test time: 1.34\n",
      "Epoch 12, train loss: 105.2879, train time: 16.13; test loss: 104.8745, test time: 1.31\n",
      "Epoch 13, train loss: 105.1351, train time: 16.34; test loss: 104.8149, test time: 1.33\n",
      "Epoch 14, train loss: 104.9285, train time: 16.13; test loss: 105.1201, test time: 1.39\n",
      "Epoch 15, train loss: 104.8878, train time: 16.26; test loss: 105.1230, test time: 1.31\n",
      "Epoch 16, train loss: 104.6577, train time: 16.35; test loss: 104.2998, test time: 1.34\n",
      "Epoch 17, train loss: 104.5238, train time: 16.31; test loss: 104.4937, test time: 1.32\n",
      "Epoch 18, train loss: 104.3852, train time: 16.33; test loss: 104.4436, test time: 1.35\n",
      "Epoch 19, train loss: 104.3283, train time: 16.20; test loss: 104.1820, test time: 1.36\n",
      "Epoch 20, train loss: 104.2305, train time: 16.25; test loss: 104.1123, test time: 1.34\n",
      "Epoch 21, train loss: 104.1458, train time: 16.23; test loss: 104.0602, test time: 1.34\n",
      "Epoch 22, train loss: 104.0362, train time: 16.14; test loss: 104.2376, test time: 1.34\n",
      "Epoch 23, train loss: 103.9918, train time: 16.34; test loss: 103.9228, test time: 1.33\n",
      "Epoch 24, train loss: 103.8744, train time: 16.29; test loss: 103.6955, test time: 1.31\n",
      "Epoch 25, train loss: 103.8613, train time: 16.21; test loss: 104.2146, test time: 1.36\n",
      "Epoch 26, train loss: 103.7248, train time: 16.27; test loss: 103.9099, test time: 1.30\n",
      "Epoch 27, train loss: 103.7092, train time: 16.24; test loss: 103.8430, test time: 1.33\n",
      "Epoch 28, train loss: 103.6203, train time: 16.24; test loss: 103.7352, test time: 1.31\n",
      "Epoch 29, train loss: 103.5853, train time: 16.21; test loss: 103.8211, test time: 1.33\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch_idx in range(EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss = train(epoch_idx)\n",
    "    train_losses.append(train_loss)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_loss = test(epoch_idx)\n",
    "    test_losses.append(test_loss)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    print('Epoch {}, train loss: {:.4f}, train time: {:.2f}; test loss: {:.4f}, test time: {:.2f}'\n",
    "          .format(epoch_idx, train_loss, train_time, test_loss, test_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_loader), len(train_loader.dataset))\n",
    "x, y = next(iter(train_loader))\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
