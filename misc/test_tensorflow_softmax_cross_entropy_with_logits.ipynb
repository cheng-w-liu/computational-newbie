{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains what is softmax and what is logit, and how to use calculate softmax cross entropy in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The softmax function is defined as:\n",
    "$\n",
    "\\text{softmax}(z)_i = \\dfrac{e^{z_i}}{\\sum_j e^{z_j}},\n",
    "$\n",
    "where $z$ is a `k-`dimension vector.\n",
    "\n",
    "In the context of the softmax, $z$ is known as the \"logits\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax entropy\n",
    "Given a probability vector `P` and the corresponding label vector `y`, the softmax cross entropy is defined as:\n",
    "\n",
    "$\n",
    "H(y, P) = -\\sum_i y^{(i)} log P^{(i)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax_cross_entropy_with_logits in TensorFlow\n",
    "The above definiton of cross entropy is defined on the label `y` and probability vector `P`. \n",
    "\n",
    "But in `tensorflow.nn.softmax_cross_entropy_with_logits`, the fuction that calculates softmax cross entropy, takes `labels` and `logits`. \n",
    "\n",
    "So, that means internally, it uses the logits to calculate the probability tensor, and then calculate the corss entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_with_logits(z, y):\n",
    "    p = softmax(z)\n",
    "    return -np.sum(np.log(p) * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.32056906819\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1.0, 0.0, 1.0, 1.0])\n",
    "z = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "cross_entropy = softmax_cross_entropy_with_logits(z, y)\n",
    "print(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.32057\n"
     ]
    }
   ],
   "source": [
    "tf_y = tf.constant(y, dtype=tf.float32)\n",
    "tf_z = tf.constant(z, dtype=tf.float32)\n",
    "tf_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf_y, logits=tf_z)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as ss:\n",
    "    ss.run(init)\n",
    "    result = ss.run(tf_cross_entropy)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
